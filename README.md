super messy, rough look into what some toy MNIST models are learning.

one result of this is the following video. it shows a small neural network with a 2-dimensional autoencoder (corresponding to the x- and y- axes of the plot) over every step on the gradient. you'll notice different labels are given different regions in this 2d space.

https://github.com/prestoj/superposition-analysis/assets/23005801/485f2777-da25-4601-8397-f657b747d0bc
